{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(\n",
    "        rescale=1./255,                    \n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "        )\n",
    "validation_data_gen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess all test images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        '/Users/roma/Desktop/Emotional_recognition/Data/FER 2013 default/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical'\n",
    "        )\n",
    "\n",
    "# Preprocess all train images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        '/Users/roma/Desktop/Emotional_recognition/Data/FER 2013 default/test',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net(optim):\n",
    "    net = Sequential(name='DCNN')\n",
    "\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(5,5),\n",
    "            input_shape=(48, 48, 1),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_1'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_1'))\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(5,5),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_2'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_2'))\n",
    "    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n",
    "    net.add(Dropout(0.4, name='dropout_1'))\n",
    "\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_3'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_3'))\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_4'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_4'))\n",
    "    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n",
    "    net.add(Dropout(0.4, name='dropout_2'))\n",
    "\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_5'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_5'))\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_6'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_6'))\n",
    "    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n",
    "    net.add(Dropout(0.5, name='dropout_3'))\n",
    "\n",
    "    net.add(Flatten(name='flatten'))\n",
    "        \n",
    "    net.add(\n",
    "        Dense(\n",
    "            128,\n",
    "            activation='elu',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='dense_1'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_7'))\n",
    "    \n",
    "    net.add(Dropout(0.6, name='dropout_4'))\n",
    "    \n",
    "    net.add(\n",
    "        Dense(7,\n",
    "            activation='softmax',\n",
    "            name='out_layer'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    net.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer= optim,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    net.summary()\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.00005,\n",
    "    patience=11,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    lr_scheduler,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(\n",
    "        rescale=1./255,                    \n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "        )\n",
    "validation_data_gen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7215 images belonging to 1 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all test images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        '/Users/roma/Desktop/Final Project/Model 3/Data/FER 2013 default/train', # your path of FER 2013 Dataset\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical'\n",
    "        )\n",
    "\n",
    "# Preprocess all train images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        '/Users/roma/Desktop/Final Project/Model 3/Data/FER 2013 default/test', # your path of FER 2013 Dataset\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DCNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        1664      \n",
      "                                                                 \n",
      " batchnorm_1 (BatchNormaliza  (None, 48, 48, 64)       256       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "                                                                 \n",
      " batchnorm_2 (BatchNormaliza  (None, 48, 48, 64)       256       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " maxpool2d_1 (MaxPooling2D)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batchnorm_3 (BatchNormaliza  (None, 24, 24, 128)      512       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batchnorm_4 (BatchNormaliza  (None, 24, 24, 128)      512       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " maxpool2d_2 (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batchnorm_5 (BatchNormaliza  (None, 12, 12, 256)      1024      \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batchnorm_6 (BatchNormaliza  (None, 12, 12, 256)      1024      \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " maxpool2d_3 (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               1179776   \n",
      "                                                                 \n",
      " batchnorm_7 (BatchNormaliza  (None, 128)              512       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,395,591\n",
      "Trainable params: 2,393,543\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 11:32:26.947499: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/folders/cb/nv527ffd4sn4f92077l2twqr0000gn/T/ipykernel_1383/1936430731.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "448/448 [==============================] - 461s 1s/step - loss: 1.9658 - accuracy: 0.3094 - val_loss: 1.5871 - val_accuracy: 0.4015 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "448/448 [==============================] - 465s 1s/step - loss: 1.4796 - accuracy: 0.4324 - val_loss: 1.3697 - val_accuracy: 0.4708 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "448/448 [==============================] - 459s 1s/step - loss: 1.3427 - accuracy: 0.4910 - val_loss: 1.2492 - val_accuracy: 0.5243 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "448/448 [==============================] - 471s 1s/step - loss: 1.2647 - accuracy: 0.5198 - val_loss: 1.2057 - val_accuracy: 0.5453 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "448/448 [==============================] - 464s 1s/step - loss: 1.2119 - accuracy: 0.5452 - val_loss: 1.1372 - val_accuracy: 0.5677 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "448/448 [==============================] - 462s 1s/step - loss: 1.1638 - accuracy: 0.5630 - val_loss: 1.1324 - val_accuracy: 0.5696 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "448/448 [==============================] - 463s 1s/step - loss: 1.1144 - accuracy: 0.5803 - val_loss: 1.0948 - val_accuracy: 0.5861 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "448/448 [==============================] - 459s 1s/step - loss: 1.0741 - accuracy: 0.6003 - val_loss: 1.0681 - val_accuracy: 0.5978 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "448/448 [==============================] - 453s 1s/step - loss: 1.0403 - accuracy: 0.6187 - val_loss: 1.0731 - val_accuracy: 0.6004 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "448/448 [==============================] - 452s 1s/step - loss: 0.9981 - accuracy: 0.6342 - val_loss: 1.0394 - val_accuracy: 0.6154 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "448/448 [==============================] - 453s 1s/step - loss: 0.9654 - accuracy: 0.6419 - val_loss: 1.0540 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "448/448 [==============================] - 454s 1s/step - loss: 0.9215 - accuracy: 0.6612 - val_loss: 1.0492 - val_accuracy: 0.6215 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "448/448 [==============================] - 456s 1s/step - loss: 0.8903 - accuracy: 0.6723 - val_loss: 1.0279 - val_accuracy: 0.6244 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "448/448 [==============================] - 458s 1s/step - loss: 0.8468 - accuracy: 0.6913 - val_loss: 1.0488 - val_accuracy: 0.6263 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "448/448 [==============================] - 456s 1s/step - loss: 0.8166 - accuracy: 0.7026 - val_loss: 1.0432 - val_accuracy: 0.6362 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "448/448 [==============================] - 457s 1s/step - loss: 0.7816 - accuracy: 0.7151 - val_loss: 1.0190 - val_accuracy: 0.6392 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "448/448 [==============================] - 461s 1s/step - loss: 0.7390 - accuracy: 0.7320 - val_loss: 1.0457 - val_accuracy: 0.6398 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "448/448 [==============================] - 460s 1s/step - loss: 0.7083 - accuracy: 0.7389 - val_loss: 1.1128 - val_accuracy: 0.6348 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "448/448 [==============================] - 470s 1s/step - loss: 0.6864 - accuracy: 0.7514 - val_loss: 1.0741 - val_accuracy: 0.6449 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "448/448 [==============================] - 468s 1s/step - loss: 0.6538 - accuracy: 0.7628 - val_loss: 1.0358 - val_accuracy: 0.6539 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "448/448 [==============================] - 477s 1s/step - loss: 0.6356 - accuracy: 0.7707 - val_loss: 1.0794 - val_accuracy: 0.6479 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "448/448 [==============================] - 486s 1s/step - loss: 0.6076 - accuracy: 0.7786 - val_loss: 1.1359 - val_accuracy: 0.6367 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "448/448 [==============================] - 496s 1s/step - loss: 0.5710 - accuracy: 0.7939 - val_loss: 1.1177 - val_accuracy: 0.6455 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "448/448 [==============================] - 511s 1s/step - loss: 0.5507 - accuracy: 0.8023 - val_loss: 1.0869 - val_accuracy: 0.6542 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "448/448 [==============================] - 480s 1s/step - loss: 0.5237 - accuracy: 0.8145 - val_loss: 1.1073 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "448/448 [==============================] - 456s 1s/step - loss: 0.5035 - accuracy: 0.8196 - val_loss: 1.1314 - val_accuracy: 0.6526 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "448/448 [==============================] - 463s 1s/step - loss: 0.4972 - accuracy: 0.8224 - val_loss: 1.0997 - val_accuracy: 0.6599 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "448/448 [==============================] - 468s 1s/step - loss: 0.4744 - accuracy: 0.8301 - val_loss: 1.1337 - val_accuracy: 0.6554 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "448/448 [==============================] - 476s 1s/step - loss: 0.4557 - accuracy: 0.8363 - val_loss: 1.1733 - val_accuracy: 0.6576 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "448/448 [==============================] - 458s 1s/step - loss: 0.4447 - accuracy: 0.8413 - val_loss: 1.1481 - val_accuracy: 0.6536 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "448/448 [==============================] - 457s 1s/step - loss: 0.4244 - accuracy: 0.8472 - val_loss: 1.1549 - val_accuracy: 0.6659 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "448/448 [==============================] - 450s 1s/step - loss: 0.4122 - accuracy: 0.8529 - val_loss: 1.2428 - val_accuracy: 0.6536 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "448/448 [==============================] - 450s 1s/step - loss: 0.3948 - accuracy: 0.8597 - val_loss: 1.2495 - val_accuracy: 0.6526 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "448/448 [==============================] - 454s 1s/step - loss: 0.3920 - accuracy: 0.8625 - val_loss: 1.2393 - val_accuracy: 0.6544 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "448/448 [==============================] - 450s 1s/step - loss: 0.3769 - accuracy: 0.8664 - val_loss: 1.2733 - val_accuracy: 0.6620 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "448/448 [==============================] - 451s 1s/step - loss: 0.3616 - accuracy: 0.8709 - val_loss: 1.2916 - val_accuracy: 0.6631 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "448/448 [==============================] - 451s 1s/step - loss: 0.3530 - accuracy: 0.8754 - val_loss: 1.3101 - val_accuracy: 0.6556 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.8790\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "448/448 [==============================] - 446s 997ms/step - loss: 0.3422 - accuracy: 0.8790 - val_loss: 1.3305 - val_accuracy: 0.6622 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "448/448 [==============================] - 452s 1s/step - loss: 0.3006 - accuracy: 0.8950 - val_loss: 1.2957 - val_accuracy: 0.6629 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "448/448 [==============================] - 445s 993ms/step - loss: 0.2682 - accuracy: 0.9035 - val_loss: 1.3341 - val_accuracy: 0.6617 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "448/448 [==============================] - 454s 1s/step - loss: 0.2511 - accuracy: 0.9102 - val_loss: 1.3619 - val_accuracy: 0.6632 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.9149Restoring model weights from the end of the best epoch: 31.\n",
      "448/448 [==============================] - 463s 1s/step - loss: 0.2439 - accuracy: 0.9149 - val_loss: 1.3780 - val_accuracy: 0.6650 - lr: 5.0000e-04\n",
      "Epoch 42: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network/model\n",
    "optims = [\n",
    "    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n",
    "    optimizers.Adam(0.001),\n",
    "]\n",
    "model = build_net(optims[1])\n",
    "emotion_model_info = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=200,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64,\n",
    "        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model structure in jason file\n",
    "model_json = model.to_json()\n",
    "with open(\"emotion_model_4.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save trained model weight in .h5 file\n",
    "model.save_weights('emotion_model_4.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
